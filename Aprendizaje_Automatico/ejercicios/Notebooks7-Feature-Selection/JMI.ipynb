{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JMI Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of the joint mutual information (JMI) approach to perform feature selection on the SRBC dataset. The performance of a nearest neighbor classifier and a naive bayes classifier will be reported. We will compare results with a variable ranking approach based on mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from a CSV file using pandas. The dataset considered is 'Simple Round Blue Cell Tumors' (SRBCT) dataset from the reference: Khan, J., Wei, J. S., Ringnér, M., Saal, L. H., Ladanyi, M., Westermann, F., … Meltzer, P. S. (2001). Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks. Nature Medicine, 7(6), 673–679. http://doi.org/10.1038/89044. This dataset has a training and a testing set, each containing 64 and 20 instances, respectively. The number of attributes is 2308, corresponding to different gene expression profiles. The number of classes is 4, corresponding to different tumors:  neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = pd.read_csv('srbct_train.csv')\n",
    "X_train = dataTrain.values[ :, 0 : (dataTrain.shape[ 1 ] - 1) ].astype(float)\n",
    "y_train = (dataTrain.values[ :, dataTrain.shape[ 1 ] - 1 ]).astype(int)\n",
    "\n",
    "dataTest = pd.read_csv('srbct_test.csv')\n",
    "X_test = dataTest.values[ :, 0 : (dataTest.shape[ 1 ] - 1) ].astype(float)\n",
    "y_test = (dataTest.values[ :, dataTest.shape[ 1 ] - 1 ]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2308)\n",
      "(20, 2308)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the train and the test data to get a single data set of 84 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((X_train, X_test))\n",
    "y = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers and Objects to Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a standard scaler and a KNN classifier. The number of neighbors to be used will be equal to 3. We will use a standard scale to preprocess the data. We also create a Naive Bayes Classifier, a a filter approach based on variable ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also set the random seed to 0, to guarantee reproducibility.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "filtering = SelectKBest(mutual_info_classif, k = 10)\n",
    "scaler = StandardScaler()\n",
    "nb = GaussianNB()\n",
    "knn= KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Filter based on JMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class that implements th JMI approach. For that, we inherit from the class BaseEstimator and TransformerMixIn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JMI(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # This is the constructor\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.S = None\n",
    "        \n",
    "    # This method will transform the data\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "\n",
    "        Xnew = X.copy()\n",
    "        return Xnew[ :, self.S ]\n",
    "    \n",
    "    # This method will perform the feature selection process of JMI\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # We use forward selection\n",
    "\n",
    "        result = []\n",
    "        S = []\n",
    "        \n",
    "        # We compute initial mutual information\n",
    "\n",
    "        mu_target = mutual_info_classif(X, y)\n",
    "        \n",
    "        # We loop over the featuers that we have to select\n",
    "\n",
    "        for n in range(self.n_features):\n",
    "            \n",
    "            sys.stdout.write('*')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            Jmax = -1e10\n",
    "            to_add = None\n",
    "            \n",
    "            # We test each feature and evaluat the criterium J.\n",
    "\n",
    "            for j in range(X.shape[ 1 ]):\n",
    "\n",
    "                if j not in S:\n",
    "\n",
    "                    if len(S) == 0:\n",
    "                        J = mu_target[ j ]\n",
    "                    else:\n",
    "                        \n",
    "                        # Compute conditional mutual information\n",
    "\n",
    "                        mu_S_cond = np.zeros(len(S))\n",
    "\n",
    "                        for y_value in range(np.max(y) + 1):\n",
    "\n",
    "                            sel = y == y_value\n",
    "                            X_sel = X[ sel, : ]\n",
    "\n",
    "                            mu_S_cond += mutual_info_regression(X_sel[ :,  S ].reshape((X_sel.shape[ 0 ], \\\n",
    "                                len(S))), X_sel[ :, j ]) * float(1.0 * X_sel.shape[ 0 ]) / X.shape[ 0 ]\n",
    "\n",
    "                        mu_S_cond = np.mean(mu_S_cond)\n",
    "\n",
    "                        # Compute mutual information\n",
    "\n",
    "                        mu_S = np.mean(mutual_info_regression(X[ :, S ].reshape((X.shape[ 0 ], len(S))), X[ :, j ]))\n",
    "\n",
    "                        J = mu_target[ j ] - (mu_S - mu_S_cond)\n",
    "\n",
    "                    # If there is an improvement we store this as the best feature    \n",
    "                        \n",
    "                    if J > Jmax:\n",
    "                        Jmax = J\n",
    "                        to_add = j\n",
    "\n",
    "            # We store the best feature found\n",
    "                        \n",
    "            S.append(to_add)\n",
    "\n",
    "        self.S = S\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of the JMI Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the objects that implement the JMI approach. This filtering method is very expensive due to the cost of evaluating the mutual information between variables and the class labels. Therefore, we perform first a filtering approach based on variable ranking that will keep only 20% of the features. After that, JMI will pick up 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_jmi = SelectKBest(mutual_info_classif, k = int(np.round(X.shape[ 1 ] * 0.2)))\n",
    "jmi =  JMI(n_features = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Process to Estimate the Generalization Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry out a 10-fold cross validation process to estimate the prediction performance of the KNN classifier as a function of the number of features considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of times the 10-fold cv process will be repeated\n",
    "\n",
    "n_repeats = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rkf = RepeatedKFold(n_splits=10, n_repeats = n_repeats, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an array to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_nb_vr = np.zeros(10 * n_repeats)\n",
    "errors_knn_vr = np.zeros(10 * n_repeats)\n",
    "errors_nb_jmi = np.zeros(10 * n_repeats)\n",
    "errors_knn_jmi = np.zeros(10 * n_repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no do the loop over the data partitions. This will take some time due to the cost of estimating mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........."
     ]
    }
   ],
   "source": [
    "# First, a simple variable ranking filtering approach\n",
    "\n",
    "split = 0\n",
    "\n",
    "for train_index, test_index in rkf.split(X, y):\n",
    "\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # First simple variable ranking\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # We standardize the data to have zero mean and unit std\n",
    "\n",
    "    scaler.fit(X_train, y_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # We filter the data using variable ranking\n",
    "\n",
    "    filtering.fit(X_train, y_train)\n",
    "    X_train_vr = filtering.transform(X_train)\n",
    "    X_test_vr = filtering.transform(X_test)\n",
    "    \n",
    "    # We fit the classifiers and compute the test performance\n",
    "\n",
    "    nb.fit(X_train_vr, y_train)\n",
    "    knn.fit(X_train_vr, y_train)\n",
    "\n",
    "    errors_nb_vr[ split ] = 1.0 - np.mean(nb.predict(X_test_vr) == y_test)\n",
    "    errors_knn_vr[ split ] = 1.0 - np.mean(knn.predict(X_test_vr) == y_test)\n",
    "\n",
    "    split += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".**********.**********.**********.**********.**********.**********.**********.**********.**********.**********"
     ]
    }
   ],
   "source": [
    "# Now JMI after an initial variable ranking filtering approach\n",
    "\n",
    "np.random.seed(0)\n",
    "    \n",
    "split = 0\n",
    "\n",
    "for train_index, test_index in rkf.split(X, y):\n",
    "\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # We standardize the data to have zero mean and unit std\n",
    "\n",
    "    scaler.fit(X_train, y_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # We filter the data first using variable ranking\n",
    "    \n",
    "    filtering_jmi.fit(X_train, y_train)\n",
    "    X_train_jmi = filtering_jmi.transform(X_train)\n",
    "    X_test_jmi = filtering_jmi.transform(X_test)\n",
    "    \n",
    "    # We filter the data again using JMI\n",
    "\n",
    "    jmi.fit(X_train_jmi, y_train)\n",
    "    X_train_jmi = jmi.transform(X_train_jmi)\n",
    "    X_test_jmi = jmi.transform(X_test_jmi)\n",
    "    \n",
    "    # We fit the classifiers and compute the test performance\n",
    "\n",
    "    nb.fit(X_train_jmi, y_train)\n",
    "    knn.fit(X_train_jmi, y_train)\n",
    "\n",
    "    errors_nb_jmi[ split ] = 1.0 - np.mean(nb.predict(X_test_jmi) == y_test)\n",
    "    errors_knn_jmi[ split ] = 1.0 - np.mean(knn.predict(X_test_jmi) == y_test)\n",
    "    \n",
    "    split += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting the Results Obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the performance of the classifier in terms of the feature selection method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With Variable Ranking Feature Selection\n",
      "Mean Error Naive Bayes:0.083333\n",
      "\tStd Mean Error Naive Bayes:0.017347\n",
      "Mean Error KNN:0.036111\n",
      "\tStd Mean Error KNN:0.017480\n",
      "\n",
      "With Variable ranking and JMI Feature Selection\n",
      "Mean Error Naive Bayes:0.058333\n",
      "\tStd Mean Error Naive Bayes:0.018509\n",
      "Mean Error KNN:0.036111\n",
      "\tStd Mean Error KNN:0.017480\n"
     ]
    }
   ],
   "source": [
    "# First simple variable ranking\n",
    "\n",
    "print(\"\\nWith Variable Ranking Feature Selection\")\n",
    "print(\"Mean Error Naive Bayes:%f\" % np.mean(errors_nb_vr))\n",
    "print(\"\\tStd Mean Error Naive Bayes:%f\" % (np.std(errors_nb_vr) / np.sqrt(len(errors_nb_vr))))\n",
    "print(\"Mean Error KNN:%f\" % np.mean(errors_knn_vr))\n",
    "print(\"\\tStd Mean Error KNN:%f\" % (np.std(errors_knn_vr) / np.sqrt(len(errors_knn_vr))))\n",
    "\n",
    "# Next, the JMI approach\n",
    "\n",
    "print(\"\\nWith Variable ranking and JMI Feature Selection\")\n",
    "print(\"Mean Error Naive Bayes:%f\" % np.mean(errors_nb_jmi))\n",
    "print(\"\\tStd Mean Error Naive Bayes:%f\" % (np.std(errors_nb_jmi) / np.sqrt(len(errors_nb_jmi))))\n",
    "print(\"Mean Error KNN:%f\" % np.mean(errors_knn_jmi))\n",
    "print(\"\\tStd Mean Error KNN:%f\" % (np.std(errors_knn_jmi) / np.sqrt(len(errors_knn_jmi))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
