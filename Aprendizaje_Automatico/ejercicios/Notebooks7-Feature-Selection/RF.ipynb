{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Feature Selection Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of Random Forest to perform feature selection on the SRBC dataset. The performance of a nearest neighbor classifier and a naive bayes classifier will be reported. We will compare results with a variable ranking approach based on mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from a CSV file using pandas. The dataset considered is 'Simple Round Blue Cell Tumors' (SRBCT) dataset from the reference: Khan, J., Wei, J. S., Ringnér, M., Saal, L. H., Ladanyi, M., Westermann, F., … Meltzer, P. S. (2001). Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks. Nature Medicine, 7(6), 673–679. http://doi.org/10.1038/89044. This dataset has a training and a testing set, each containing 64 and 20 instances, respectively. The number of attributes is 2308, corresponding to different gene expression profiles. The number of classes is 4, corresponding to different tumors:  neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = pd.read_csv('srbct_train.csv')\n",
    "X_train = dataTrain.values[ :, 0 : (dataTrain.shape[ 1 ] - 1) ].astype(float)\n",
    "y_train = (dataTrain.values[ :, dataTrain.shape[ 1 ] - 1 ]).astype(int)\n",
    "\n",
    "dataTest = pd.read_csv('srbct_test.csv')\n",
    "X_test = dataTest.values[ :, 0 : (dataTest.shape[ 1 ] - 1) ].astype(float)\n",
    "y_test = (dataTest.values[ :, dataTest.shape[ 1 ] - 1 ]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2308)\n",
      "(20, 2308)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the train and the test data to get a single data set of 84 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((X_train, X_test))\n",
    "y = np.hstack((y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers and Objects to Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a standard scaler and a KNN classifier. The number of neighbors to be used will be equal to 3. We will use a standard scale to preprocess the data. We also create a Naive Bayes Classifier, a a filter approach based on variable ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also set the random seed to 0, to guarantee reproducibility.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "filtering = SelectKBest(mutual_info_classif, k = 10)\n",
    "scaler = StandardScaler()\n",
    "nb = GaussianNB()\n",
    "knn= KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of the Feature Selection Method Based on Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the objects that implement the feature selection approach based on Random Forest. This filtering method can be very expensive due to the need of building a big ensemble of decision trees. Therefore, we perform first a filtering approach based on variable ranking that will keep only 20% of the features. After that, Random Forest will pick up 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_rf = SelectKBest(mutual_info_classif, k = int(np.round(X.shape[ 1 ] * 0.2)))\n",
    "\n",
    "# We specify a threshold value equal to zero. Then it will be increased to choose only 10 feautres.\n",
    "\n",
    "rf_selection =  SelectFromModel(RandomForestClassifier(n_estimators = 2000, \\\n",
    "    random_state = 0), threshold = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Process to Estimate the Generalization Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry out a 10-fold cross validation process to estimate the prediction performance of the KNN classifier as a function of the number of features considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of times the 10-fold cv process will be repeated\n",
    "\n",
    "n_repeats = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rkf = RepeatedKFold(n_splits=10, n_repeats = n_repeats, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an array to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_nb_vr = np.zeros(10 * n_repeats)\n",
    "errors_knn_vr = np.zeros(10 * n_repeats)\n",
    "errors_nb_rf = np.zeros(10 * n_repeats)\n",
    "errors_knn_rf = np.zeros(10 * n_repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no do the loop over the data partitions. This will take some time due to the cost of estimating mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........."
     ]
    }
   ],
   "source": [
    "# First, a simple variable ranking filtering approach\n",
    "\n",
    "split = 0\n",
    "\n",
    "for train_index, test_index in rkf.split(X, y):\n",
    "\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # First simple variable ranking\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # We standardize the data to have zero mean and unit std\n",
    "\n",
    "    scaler.fit(X_train, y_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # We filter the data using variable ranking\n",
    "\n",
    "    filtering.fit(X_train, y_train)\n",
    "    X_train_vr = filtering.transform(X_train)\n",
    "    X_test_vr = filtering.transform(X_test)\n",
    "    \n",
    "    # We fit the classifiers and compute the test performance\n",
    "\n",
    "    nb.fit(X_train_vr, y_train)\n",
    "    knn.fit(X_train_vr, y_train)\n",
    "\n",
    "    errors_nb_vr[ split ] = 1.0 - np.mean(nb.predict(X_test_vr) == y_test)\n",
    "    errors_knn_vr[ split ] = 1.0 - np.mean(knn.predict(X_test_vr) == y_test)\n",
    "\n",
    "    split += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........."
     ]
    }
   ],
   "source": [
    "# Now RF after an initial variable ranking filtering approach\n",
    "\n",
    "np.random.seed(0)\n",
    "    \n",
    "split = 0\n",
    "\n",
    "for train_index, test_index in rkf.split(X, y):\n",
    "\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # We standardize the data to have zero mean and unit std\n",
    "\n",
    "    scaler.fit(X_train, y_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # We filter the data first using variable ranking\n",
    "    \n",
    "    filtering_rf.fit(X_train, y_train)\n",
    "    X_train_rf = filtering_rf.transform(X_train)\n",
    "    X_test_rf = filtering_rf.transform(X_test)\n",
    "    \n",
    "    # We filter the data again using RF\n",
    "    \n",
    "    rf_selection.fit(X_train_rf, y_train)\n",
    "    rf_selection.threshold = -1.0 * np.sort(-1.0 * rf_selection.estimator_.feature_importances_)[ 9 ]\n",
    "    X_train_rf = rf_selection.transform(X_train_rf)\n",
    "    X_test_rf = rf_selection.transform(X_test_rf)\n",
    "    \n",
    "    # We fit the classifiers and compute the test performance\n",
    "\n",
    "    nb.fit(X_train_rf, y_train)\n",
    "    knn.fit(X_train_rf, y_train)\n",
    "\n",
    "    errors_nb_rf[ split ] = 1.0 - np.mean(nb.predict(X_test_rf) == y_test)\n",
    "    errors_knn_rf[ split ] = 1.0 - np.mean(knn.predict(X_test_rf) == y_test)\n",
    "    \n",
    "    split += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting the Results Obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the performance of the classifier in terms of the feature selection method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With Variable Ranking Feature Selection\n",
      "Mean Error Naive Bayes:0.083333\n",
      "\tStd Mean Error Naive Bayes:0.017347\n",
      "Mean Error KNN:0.036111\n",
      "\tStd Mean Error KNN:0.017480\n",
      "\n",
      "With Variable ranking and RF Feature Selection\n",
      "Mean Error Naive Bayes:0.025000\n",
      "\tStd Mean Error Naive Bayes:0.015811\n",
      "Mean Error KNN:0.034722\n",
      "\tStd Mean Error KNN:0.023011\n"
     ]
    }
   ],
   "source": [
    "# First simple variable ranking\n",
    "\n",
    "print(\"\\nWith Variable Ranking Feature Selection\")\n",
    "print(\"Mean Error Naive Bayes:%f\" % np.mean(errors_nb_vr))\n",
    "print(\"\\tStd Mean Error Naive Bayes:%f\" % (np.std(errors_nb_vr) / np.sqrt(len(errors_nb_vr))))\n",
    "print(\"Mean Error KNN:%f\" % np.mean(errors_knn_vr))\n",
    "print(\"\\tStd Mean Error KNN:%f\" % (np.std(errors_knn_vr) / np.sqrt(len(errors_knn_vr))))\n",
    "\n",
    "# Next, the RF approach\n",
    "\n",
    "print(\"\\nWith Variable ranking and RF Feature Selection\")\n",
    "print(\"Mean Error Naive Bayes:%f\" % np.mean(errors_nb_rf))\n",
    "print(\"\\tStd Mean Error Naive Bayes:%f\" % (np.std(errors_nb_rf) / np.sqrt(len(errors_nb_rf))))\n",
    "print(\"Mean Error KNN:%f\" % np.mean(errors_knn_rf))\n",
    "print(\"\\tStd Mean Error KNN:%f\" % (np.std(errors_knn_rf) / np.sqrt(len(errors_knn_rf))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
