{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100c23e1",
   "metadata": {},
   "source": [
    "<div class=\"title\">Non-Linear Models and SVMs: Review</div>\n",
    "<div class=\"subtitle\">Machine Learning</div>\n",
    "<div class=\"author\">Carlos María Alaíz Gudín &mdash; Universidad Autónoma de Madrid</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac0928",
   "metadata": {},
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the general configuration of Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<head><link rel=\"stylesheet\" href=\"style.css\"></head>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the packages to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages.\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "\n",
    "# Initialisations.\n",
    "matplotlib.rc(\"figure\", figsize=(15, 5))\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2c0f2",
   "metadata": {},
   "source": [
    "# Review of Non-Linear Models and SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generalized Linear Model.\n",
    "\n",
    "\n",
    "* Non-Linear Mapping or Embedding.\n",
    "\n",
    "\n",
    "* Feature Construction.\n",
    "\n",
    "\n",
    "* Set of Basis Functions.\n",
    "\n",
    "\n",
    "* Adaptive Basis Functions.\n",
    "\n",
    "\n",
    "* Dual Problem.\n",
    "\n",
    "\n",
    "* Kernel Trick.\n",
    "\n",
    "\n",
    "* Kernel Function.\n",
    "\n",
    "\n",
    "* Kernel Ridge Regression.\n",
    "\n",
    "\n",
    "* Margin.\n",
    "\n",
    "\n",
    "* Hard-Margin Support Vector Machine.\n",
    "\n",
    "\n",
    "* Dual Formulation.\n",
    "\n",
    "\n",
    "* Support Vector.\n",
    "\n",
    "\n",
    "* Soft-Margin Support Vector Machine.\n",
    "\n",
    "\n",
    "* Hinge Loss Function.\n",
    "\n",
    "\n",
    "* Support Vector Regression.\n",
    "\n",
    "\n",
    "* $\\epsilon$-Insensitive Loss.\n",
    "\n",
    "\n",
    "* RBF or Gaussian Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the generalized linear model built over the mapping\n",
    "$$ \\boldsymbol{\\phi}(x_1, x_2) = (x_1^2, x_2^2, x_1 x_2 + 2) , $$\n",
    "and with parameters $\\boldsymbol{\\theta} = \\{ b = 2, \\mathbf{w} = (1, 2, 3)^\\intercal \\}$, and given the following dataset:\n",
    "\n",
    "| $$x_{i, 1}$$ | $$x_{i, 2}$$ | $$y_i$$ |\n",
    "|--------|---------|--------|\n",
    "|    3   |    2    |   40   |\n",
    "|    1   |    4    |   55   |\n",
    "|    0   |    0    |   10   |\n",
    "\n",
    "1. Compute the Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Insert code.\n",
    "def map_data(X):\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    return np.column_stack((x1**2, x2**2, x1 * x2 + 2))\n",
    "\n",
    "\n",
    "b = 2\n",
    "w = np.array([1, 2, 3])\n",
    "\n",
    "x = np.array([[3, 2], [1, 4], [0, 0]])\n",
    "y = np.array([40, 55, 10])\n",
    "\n",
    "Phi = map_data(x)\n",
    "pred = Phi @ w + b\n",
    "\n",
    "for i in range(len(x)):\n",
    "    print(\"\\nx = \", x[i], \"\\nphi(x) = \", Phi[i], \"\\nf(x) = \", pred[i])\n",
    "\n",
    "print(\"\\nMSE: {:.2f}\".format(mean_squared_error(y, pred)))\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin of a Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 2-dimensional linear classification model with parameters $\\boldsymbol{\\theta} = \\{ b = -1.25, \\mathbf{w} = (1, 1)^\\intercal \\}$, and given the following dataset:\n",
    "\n",
    "| $$x_{i, 1}$$ | $$x_{i, 2}$$ |\n",
    "|--------|---------|\n",
    "|    1   |    0    |\n",
    "|    0   |    1    |\n",
    "|    1   |    1    |\n",
    "\n",
    "1. Compute the margin of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Insert code.\n",
    "b = -1.25\n",
    "w = np.array([1, 1])\n",
    "\n",
    "x = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "\n",
    "n_w = np.linalg.norm(w)\n",
    "margin = np.inf\n",
    "for i in range(len(x)):\n",
    "    pred = x[i] @ w + b\n",
    "    dist = np.abs(pred) / n_w\n",
    "    margin = min(dist, margin)\n",
    "\n",
    "    print(\"x        = \", x[i])\n",
    "    print(\"Distance = {:.2f}\\n\".format(dist))\n",
    "\n",
    "print(\"\\n\\tMargin: {:.2f}\".format(margin))\n",
    "\n",
    "xlim = np.array([-0.5, 1.5])\n",
    "plt.plot(xlim, (-b - xlim * w[0]) / w[1], \"--\")\n",
    "plt.scatter(x[:, 0], x[:, 1])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 2-dimensional linear classification model with parameters $\\boldsymbol{\\theta} = \\{ b = -2.5, \\mathbf{w} = (2, 2)^\\intercal \\}$, and given the following dataset:\n",
    "\n",
    "| $$x_{i, 1}$$ | $$x_{i, 2}$$ | $$y_i$$ |\n",
    "|--------|---------|--------|\n",
    "|    1   |    0    |   -1   |\n",
    "|    0   |    1    |    1   |\n",
    "|    1   |    1    |    1   |\n",
    "\n",
    "1. Compute the hinge loss error for each pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Insert code.\n",
    "b = -2.5\n",
    "w = np.array([2, 2])\n",
    "\n",
    "x = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([-1, 1, 1])\n",
    "\n",
    "for i in range(len(x)):\n",
    "    pred = x[i] @ w + b\n",
    "    loss = np.maximum(1 - y[i] * pred, 0)\n",
    "\n",
    "    print(\"x     = \", x[i])\n",
    "    print(\"Error = {:.2f}\\n\".format(loss))\n",
    "\n",
    "xlim = np.array([-0.5, 1.5])\n",
    "plt.plot(xlim, (-b - xlim * w[0]) / w[1], \"--\")\n",
    "plt.plot(xlim, (-b - xlim * w[0] + 1) / w[1], \":k\")\n",
    "plt.plot(xlim, (-b - xlim * w[0] - 1) / w[1], \":k\")\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Insensitive Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 2-dimensional linear regression model with parameters $\\boldsymbol{\\theta} = \\{ b = -2.5, \\mathbf{w} = (2, 2)^\\intercal \\}$, and given the following dataset:\n",
    "\n",
    "| $$x_{i, 1}$$ | $$x_{i, 2}$$ | $$y_i$$ |\n",
    "|--------|---------|--------|\n",
    "|    1   |    0    |   -1   |\n",
    "|    0   |    1    |    0   |\n",
    "|    1   |    1    |    1.4 |\n",
    "\n",
    "1. Compute the $\\epsilon$-insensitive loss error for each pattern, for $\\epsilon = 0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Insert code.\n",
    "b = -2.5\n",
    "w = np.array([2, 2])\n",
    "epsilon = 0.2\n",
    "\n",
    "x = np.array([[1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([-1, 0, 1.4])\n",
    "\n",
    "for i in range(len(x)):\n",
    "    pred = x[i] @ w + b\n",
    "    diff = np.abs(pred - y[i])\n",
    "    loss = np.maximum(diff - epsilon, 0)\n",
    "\n",
    "    print(\"x     = \", x[i])\n",
    "    print(\"y     = {:.2f}\".format(y[i]))\n",
    "    print(\"f(x)  = {:.2f}\".format(pred))\n",
    "    print(\"Error = {:.2f}\\n\".format(loss))\n",
    "################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
