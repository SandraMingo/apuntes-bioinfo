%05/02 - Fátima Sánchez Cabo
\part{Transcriptómica}
\chapter{RNA-Seq}
\section{Pipeline general y alineadores}
En este curso nos centraremos en los NGS de lectura corta (segunda generación). Para transcriptómica, se secuencia el cDNA generado a partir del ARNm. Este cDNA se fragmenta y se secuencia en reads cortos. Las máquinas y la forma de secuenciar es la misma que aquella vista en la asignatura "Fundamentos de Secuenciación". 

Las lecturas pueden ser solo de la primera parte del fragmento (single-reads) o lecturas pareadas para tener una mayor precisión en el alineamiento. Del secuenciador sale un fichero FastQ, el cual se alinea con un genoma de referencia en FastA. Una vez con los alineamientos, se pueden mirar las regiones con reads mapeadas que estén en el transcriptoma de referencia (GTF/GFF) y cuantificar la expresión (matriz de conteo en CSV/TSV). 

En general, el workflow es el siguiente: descargar datos, QC inicial, pre-procesamiento y QC posterior, alineamiento a una referencia, detección, conteo y análisis estadístico. En transcriptómica, se quiere cuantificar la expresión de las partes del genoma que se transcriben. Por ello, se necesita un genoma de referencia, pero también otro archivo GTF que relacione los exones con los tránscritos y los genes. El contaje por detección es el que se hace con CHIP-Seq, al secuenciar la parte del ADN genómico a la que se ha pegado un factor de transcripción predefinido. 

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/ngs-analysis-workflow.png}
\end{figure}

\subsection{Control de calidad inicial}
Los objetivos del control de calidad de las lecturas crudas es detectar problemas de secuenciación, detectar adaptadores y comparar librerías para análisis posteriores. Distintos experimentos requieren interpretaciones distintas del análisis de control de calidad. Una herramienta muy utilizada para esto es FastQC. El análisis de la calidad por base, representa la distribución de las puntuaciones de calidad en todas las lecturas por la posición de cada lectura. En general, es normal que las últimas posiciones tengan una calidad algo peor que las demás, pero una buena muestra debe seguir teniendo una calidad alta. Si una muestra no tiene gran calidad, se puede optar por utilizar solo aquella porción de las muestras que tienen una calidad aceptable, pero hay que tener en cuenta que al acortar las lecturas, el mapeado puede darse en un mayor número de sitios. También se mide el contenido de cada base por posición (que debería ser bastante constante a lo largo de toda la lectura dependiendo de la "complejidad de la muestra", es decir, variedad de tránscritos diferentes) y la puntuación de calidad media por secuencia. La pipeline para el ARNm y los miRNA es la misma, pero hay peculiaridades. Los microARNs son ARNs de unos 20-30 nucleótidos que reprimen la expresión génica de los genes a los que se unen.
Dado el bajo número de miRNAs codificados por el genoma y el más reducido número expresado en cada tejido es de esperar ver perfiles de baja complejidad en las librerías de miRNAs, dádose así un patrón irregular del contenido de bases por posición.

Las secuencias sobrerrepresentadas son listas de secuencias que están presentes más veces de lo esperado por azar. La lista sobrerrepresentada se anota con el tipo de secuencia si se proporciona una lista con la que comparar. Normalmente, las secuencias adaptadoras pueden estar sobrerrepresentadas si hay altos niveles de ligaciones de dímeros de cebadores en el paso de preparación de la biblioteca. A menudo se debe a un desequilibrio entre los niveles de adaptadores y los niveles de fragmentos de muestra. Algunos RNA-Seq de tejidos particulares pueden dar también secuencias sobrerrepresentadas. Por ejemplo, las muestras de sangre contienen grandes cantidades de transcritos de hemoglobina que siempre se reportan como lecturas sobrerrepresentadas. Las muestras de miARN siempre muestran secuencias sobrerrepresentadas. Las bibliotecas de ARN total muestran secuencias sobrerrepresentadas de ARN ribosómicos.

\subsection{Preprocesado}
El preprocesado tiene como objetivo mejorar la calidad, la mapeabilidad, quitar contaminantes y sesgos, etc. Hay diferentes herramientas, como cutadapt o trim-galore. 

La calidad de las bases puede afectar al análisis de llamadas de variantes y, si es grave, también al mapeo de características. La calidad de las bases suele disminuir al final de la lectura y a veces al principio. Además, la secuenciación de baja calidad puede producir un grupo de lecturas de baja calidad a lo largo de su longitud. Es esencial eliminar las bases de baja calidad para el análisis de llamada de variantes. Para otros análisis, elimínelas sólo si afecta al rendimiento del mapeo.

\subsection{Alineamiento y mapeado}
Hay dos tipos de alineamientos: local y global. En el caso del local, se busca que en partes específicas el alineamiento sea bueno, mientras que en el global se busca meter la lectura en la secuencia completa, metiendo gaps. 

La cobertura en un segmento se mide como el número de reads que mapean a ese fragmento del genoma y la longitud de cada lectura dividido por la longitud del fragmento. Para poder hacer el mapeado se necesita la referencia en fasta, las reads en fastq y una referencia indexada. Esto es distinto de los alineadores tradicionales como BLAST. El objetivo es mapear las lecturas a las características. En transcriptómica, el alineamiento se realiza al mismo tiempo que la cuantificación. 

Lo importante es la indexación del genoma de referencia para ahorrar tiempo de computación. El genoma se corta en trozos para que sea más fácil realizar las búsquedas. Esto se puede hacer por ejemplo con BWA y alineadores Bowtie que utilizan la transformación de Burrows-Wheeler al ser más rápidos.

Aunque se hable de expresión génica, los genes no se expresan, son los tránscritos. Con estas técnicas es muy complicado hilar tan fino, por lo que se cuantifican los reads al gen y contar. A la hora de alinear, si se intenta alinear reads al genoma de referencia, las reads salen del tránscrito, por lo que puede ocurrir que una parte de un read caiga en un exón y la otra parte en el otro. Esto se puede visualizar con el visor IGV. Las lecturas partidas se conocen como \textbf{exon junctions}. Por ello, se puede mapear al genoma permitiendo esa característica. Otra opción es alinear directamente al transcriptoma, pero es más grande que el genoma (puede haber 100.000 tránscritos definidos vs 20.000 - 30.000 genes) y puede que haya lecturas que no se puedan mapear a un tránscrito concreto, si no que puedan mapear a varios tránscritos con el mismo exon.

\begin{figure}[h]
\centering
\includegraphics[width = 0.5\textwidth]{figs/Imagen1.png}
\end{figure}

\subsection{Galaxy}
Galaxy es una plataforma con muchas herramientas y workflows ya hechos para investigación biomédica intensiva en datos. Permite generar y hacer públicas pipelines. Se puede utilizar en el servidor europeo o montar un servidor local. 

Para nuestro proyecto, utilizaremos los datos del paper "Next-generation sequencing facilitates quantitative analysis of wild-type and Nrl(-/-) retinal transcriptomes". En la parte de "Related Information" se encuentran los datasets subidos a la base de datos GEO (Gene Expression Omnibus). En general, las revistas buenas exigen poner los datos en una base de datos pública. En este caso hay 6 muestras, 3 wild-type y 3 knock-out. Cada muestra en GEO tiene un ID. 

Galaxy se conecta a las bases de datos mediante API, por lo que se puede poner el link a los reads crudos y Galaxy lo lleva a nuestra sesión sin necesidad de descargarlos de las bases de datos y subirlos a Galaxy de forma manual. 

Nos vamos a descargar la información de los nombres de las muestras (los metadatos). Desde GEO, hay un acceso a SRA Run Selector donde tenemos disponibles esos datos. Hay dos tablas disponibles: metadata y lista de las accesiones con los IDs de las muestras. Los metadatos se necesita posteriormente para saber qué muestras son WT y cuáles KO, pero por ahora solo necesitamos los IDs para subir a Galaxy. El siguiente paso es decirle a Galaxy que, utilizando esos identificadores, se descarguen los FastQ. Para ello, en Get Data hay una opción de Faster Download and Extract Reads in FastQ format from NCBI SRA. Para esa herramienta se selecciona la opción de "List of SRA accessions, one per line" y se ejecuta.

Una vez con los datos, vemos que en Pair-end tenemos 0 datos y en Single-end 6, indicando que las muestras son single-end (aunque esto ya lo sabíamos porque venía en SRA Run Selector). El siguiente paso es ir a FastQC con los datos de single-end. La salida es un fichero txt con los números y un html con las imágenes. Tras analizarlo brevemente, vemos que no hay ningún problema con las muestras, pudiendo continuar con el análisis.

En Ensembl nos vamos a la página de FTP Downloads donde se encuentran todas las referencias de la última versión del genoma. Para reproducir unos resultados, hay que utilizar la referencia de la fecha de publicación de los datos que se estén utilizando, pero en nuestro caso podemos utilizar la última versión generada. Como no queremos descargar el Fasta a nuestro ordenador, vamos al FastA y buscamos el fichero de primary assembly. Con click derecho, podemos copiar el enlace y en Galaxy, en Upload, se puede utilizar la función Paste/Fetch data y pegar ahí la dirección. También subimos el GTF de los cromosomas.

%07/02 - Fátima
El siguiente paso es buscar la herramienta Trim-Galore con los datos single-end dejando todas las opciones como las predeterminadas. Con esta herramienta queremos quitar los adaptadores, y en caso de tener muestras dañadas, podríamos también eliminar esa parte. 

\section{Expresión diferencial}
\subsection{Visualización con IGV}
Para subir un fichero a IGV, nos vamos a File y Load from File. Hay que tener en la misma carpeta el fichero BAM con el fichero BAI, es decir, el fichero indexado. Hay que cargar el BAM. Se muestran tres tracks, siendo uno la cobertura, otro los junctions y el último los duplicados (click derecho y expandir). Previamente hay que seleccionar el genoma correcto; en este caso, el de ratón. 

Podemos irnos al cromosoma 13 y buscar el gen Smn1, con eso saltamos a esa región del genoma. Podemos ver las distintas isoformas del gen y dónde han mapeado las lecturas. En Junctions, vemos que hay lecturas con un arco grande, indicando que la lectura ha mapeado a esos exones que estaban juntos en el tránscrito, pese a que en el genoma estén separados por intrones y otras regiones no codificantes. La cobertura coincide con los exones al tratarse de un RNA-Seq. Además, tiene una cobertura 0-20, indicando que en ese rango hay como máximo 20 reads y como mínimo 0. 

La isoforma superior tiene un exón al principio de la proteína que no tiene ninguna lectura. Esto puede darse por la cobertura baja, indicando que nos estamos perdiendo esa isoforma.

\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{figs/igv-1.png}
\end{figure}

\subsection{Redundancia de mapeo}
Pueden darse redundancias de mapeo, ya que la secuencia del genoma es larga y contiene muchas secuencias repetitivas. Por ello, hay que mirar la calidad de mapeado, ya que una lectura puede mapear en un sitio con 1 mismatch y en otra región con 2. Para reducir la ambigüedad en el mapeado (hay genes pareados, isoformas), se puede utilizar pair-end o secuenciación de reads más largas. 

En NGS, se pueden detectar regiones por enriquecimiento viendo, en base a la cobertura del experimento, las regiones donde hay señal y que indicarán genes o factores de transcripción. El proceso es crosslinking, sonicación, inmunoprecipitación y secuenciación. El resultado de este tipo de experimento es un archivo tipo BED o WIG en el que se obtiene la posición donde se encuentra la señal.

En el conteo por ocurrencias, se cuentan cuántas reads caen en las distintas regiones. Para ello se requiere el GTF que asocia los disitntos exones con los tránscritos o isoformas.

En Galaxy, se puede utilizar un solo programa para alinear las lecturas a la referencia y la cuantificación. Al final, no importa si una read pertenece a una isoforma o a otra si queremos abstraer la cuantificación de una proteína, es decir, obtener la cuantificación absoluta. Para la expresión diferencial, se necesita más cobertura y los métodos son un poco diferentes para poder diferenciar las isoformas. Los tránscritos tienen una estructura de dependencia muy complicada, por lo que la expresión diferencial se suele hacer a nivel de gen.

\subsection{Cálculo de expresión}
Una vez con las lecturas mapeando a un gen, si queremos tener una medida robusta de la expresión, hay que tener en cuenta la longitud del gen y el tamaño de la librería. Una librería con una secuenciación mayor, la expresión va a parecer mayor que en una secuenciación con un tamaño menor de librería. Además, un tránscrito más corto va a tener menos reads que caigan en él por mera probabilidad, por lo que hay que normalizar por el tamaño del gen. Para ello, primero se obtienen los counts (la cobertura) y se utilizan los RPKMs:
$$RPKM: 10^9 \cdot \frac{\text{Reads mapped to the transcript}}{\text{Total reads} \cdot \text{Transcript length}}$$
Esta fórmula se modificó a la siguiente para normalizar todo a la vez:
$$TPM = 10^6 \cdot \frac{\text{reads mapped to transcript / transcript length}}{\sum \text{reads mapped to transcript/transcript length}}$$

Para las reads que mapean a varias isoformas o a varios genes del genoma, se utilizan los programas RSEM, Salmon o Sailfish. Estos métodos son procesos iterativos. Se aprovechan de la información de todos los reads para mejorar la probabilidad de qué read pertenece a qué sitio. Teniendo tres isoformas que coinciden en el primer exón, se ven las reads que caen en las partes distintas de los tránscritos para inferir las reads de la parte común de los tránscritos. La probabilidad se va cambiando y ajustando según cambian las probabilidades. Estos métodos probabilísticos hacen una estimación de los counts. Estos algoritmos cuantifican la expresión por isoforma, ya que están hechos para lidiar con el multimapping. La columna IsoPct indica el porcentaje de expresión de esa isoforma sobre toda la expresión del gen completo. Cuando se ve un proceso de splicing alternativo, la isoforma mayoritaria es la primera, y en otra condición puede darse que todas las isoformas estén igual de expresadas o que se convierta en la menos expresada.

El gen SMN1 de ratón es esencial, no puede haber ninguna mutación al ser inviable. De hecho, en humanos, una mutación en este gen causa SMA (spinal muscular atrophy). Esto se debe a que tenemos otro gen, SMN2, que solo se diferencia del 1 en una base y puede ayudar a compensar. Aunque las reads sean prácticamente idénticas, RSEM es capaz de asignarlas a una forma u otra.

\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{figs/smn12.png}
\end{figure}

\subsection{Galaxy}
Volviendo a la práctica, nos vamos a Ensembl y BioMart. Seleccionamos el genoma de ratón, seleccionamos como atributos solo Gene stable ID y transcript stable ID (quitamos los version) y exportamos los resultados como tsv. En Galaxy subimos ese fichero (Gene2Transcript) y utilizamos la herramienta "Sort Column Order by heading" para poner la columna 2 como identificador. 

El siguiente paso es construir la referencia del tránscrito (el fasta del transcriptoma) a partir del GTF que habíamos subido previamente con la herramienta gffread. Debería coger automática el fichero gtf, y en caso contrario lo seleccionamos manualmente. En el apartado de Reference Genome, debemos poner "From your history" para poder indicar el fasta. Además, en "select fasta outputs", seleccionamos la opción de "fasta file with spliced exons for each GFF transcript". También hay que activar "full GFF attribute preservation", y en Feature File Output poner GTF.

%12/02 - Fátima
A continuación utilizamos salmon\_qual utilizando el fichero exons.fa que se acaba de generar.
Los alineadores/mapeadores alinean los reads base a base y cuantifican por exón, tránscrito y gen el número de reads que caen en cada una de esas regiones. Por ello, debe recibir la referencia exons.fa creado a partir del GTF y del Fasta. También recibe la tabla que mapea los tránscritos a los genes (la que hemos construido con el sort column) para conseguir la cuantificación por gen. Aunque nosotros hayamos utilizado salmon, existen otros algoritmos como sailfish y RSEM. En RNA-Seq no quitamos los duplicados al poder significar una mayor expresión. Además, los tres métodos permiten el multimapping para poder ver si las reads van a una u otra isoforma mediante métodos estadísticos de expectation-maximization. 

El resultado contiene el gen (no tránscrito, eso sería otro análisis), la longitud del gen, la longitud efectiva (la que se puede mapear), los TPMs y el número de reads. Esto último es el dato crudo de cuántas reads del experimento caen en ese gen, mientras que los TPMs son los datos normalizados. Para el análisis de expresión diferencial, vamos a utilizar las reads crudas. 

\section{Análisis de expresión diferencial}
\subsection{Galaxy}
Para el análisis de la expresión diferencial, necesitamos una tabla con el ID del gen y las columnas con los reads. Esto lo vamos a hacer dentro de Galaxy con la función cut con el output de salmon y escogiendo las columnas 1 y 5. El siguiente paso es utilizar columnjoin sobre este resultado, siendo la columna 1 el identificador y con 1 línea de encabezado en cada fichero de input.

Con los counts normalizados por el tamaño de gen y de librería, si hiciéramos un plot de expresión sobre lgFC, habría más variabilidad a baja expresión. Limma-voom y trend permite estabilizar la varianza para poder realizar posteriormente el test estadístico. El ejemplo en el que estamos trabajando es bastante sencillo, pero para cuando nos veamos en situaciones más complejas (varias condiciones, medidas repeticas, etc) hay un tutorial de limma escrito por su autor, Gordon Smyth. voom permite meter información sobre la calidad de las réplicas, pero en este caso no vamos a usarlo. Seleccionamos single count matrix. El factor es la variable que determina las condiciones. Por ello, nosotros vamos a añadir un factor llamado genotype y damos para cada muestra el grupo al que pertenece. Esta información está en los metadatos; las primeras tres muestras son WT y las otras tres KO, por lo que debemos proporcionar lo siguiente: WT, WT, WT, KO, KO, KO. 

En Ensembl BioMart, seleccionamos Ensembl Genes y Mouse genes. En Attributes, debemos marcar Gene stable ID y Gene Name y obtener los resultados únicos. Con los resultados descargados, en Galaxy permitimos Gene Annotations y subimos este fichero. Este paso es opcional, es simplemente para que podamos ver mejor los genes. En contrast, debemos poner KO-WT.

A la hora de hacer el análisis de expresión diferencial, cuantas más hipótesis (genes) testemos, más habrá que corregir el p-valor. Por ello, genes no expresados, no aportan información y aumentan el error de tipo 1. Por ello, se pueden quitar los filtrados. En este caso, no queremos quitar aquellos genes que entre las dos condiciones esté downregulado (y en KO no tenga expresión, por ejemplo), pero si un gen no está expresado en más de 4 muestras (de 3 que tenemos por cada condición), sí podemos quitarlos, por lo que sí permitimos el filtrado de genes poco expresados. El filtrado se hace en base de las CPM, poniendo que haya al menos 1 count por million en al menos 3 muestras. Dentro de opciones de salida, seleccionamos todos los posibles plots. 
%esto es lo que pedirá en el ejercicio para que los interpretemos
Del output, siempre hay que usar el p-valor ajustado. 

Por detrás, se ha ejecutado un script de R, que lo veremos más en detalle a continuación. Utilizamos el script \texttt{limma\_example\_rma.r} localizado en la carpeta de prácticas. 

De Galaxy, nos descargamos la tabla resultante (localizada en carpeta de prácticas): Galaxy64-[limma-voom\_KO-WT].tabular. 
Ahora tenemos una colección de genes diferencialmente expresados y tenemos dos opciones: ir mirando uno a uno los genes (por ejemplo, Nlr tiene un p-valor ajustado de $10^{-12}$ y un logFC de -8, es un gen utilizado para el Knock-Out y verificamos que ha funcionado bien. A partir de ahora, buscaríamos rutas de genes que se verían afectados con el KO de este gen. 

Hay dos tipos de análisis funcional: 
\begin{itemize}
\item \textbf{ORA (overrepresentation analysis):} Tenemos 6407 genes diferencialmente expresados de 18387 (obtenidos mediante un filtro de p-valor ajustado < 0.5). Cada uno de los genes se puede clasificar en función del proceso biológico en el que está involucrado (biological process; BP), la función molecular (molecular function; MF) o el compartimento celular (cellular compartment; CC). Para un gen, podemos buscar esto en Gene Ontology Overview. Dentro de cada categoría, hay muchas clases que permiten clasificar los genes de forma cada vez más específica, y un gen puede estar en varias clases. Podemos escoger el número de procesos y la profundidad que deseamos. Asignamos así cada gen a un proceso, y contamos para cada proceso cuántos DEGs hay. Este número se compara con los valores de Whole genome. Aquellas proporciones muy enriquecidas son las que se marcan como cambio significativo. Así, se puede decir que el experimento afecta sobre todo al proceso x (por ejemplo, regulación génica). En caso de no obtener ningún DEG, se podrían utilizar filtros más laxos, o utilizar GSEA. 
\item \textbf{GSEA (gene set enrichment analysis):} para este método, se cogen la lista de todos los genes y se ordenan de menor a mayor en función de su logFC (aunque se puede elegir en función de qué hacer la ordenación). Seguimos teniendo la lista con los procesos a los que pertenecen los genes. Por ello, podemos ver dónde caen los genes de cada proceso. Si para un proceso todos los genes se encuentran cerca, esto se puede reportar (tienen una magnitud de cambio similar; el experimento perturba ese proceso). 
\end{itemize}
 
En el NIH está la herramienta \href{https://davidbioinformatics.nih.gov/}{DAVID} que permite subir una lista o un background (pestaña functional annotation). En el caso de los arrays, no se miran todos los genes, por lo que no tiene sentido comparar en el ORA con whole genome, si no que se utiliza otro background (por ejemplo, solo el cromosoma 1, lo que se utilice como total). En nuestro caso copiamos los primeros 150 gene ID, pero también se podría subir en forma de fichero. Se selecciona que los identificadores sean de genes de Ensembl y que se trata de una lista, no background. Hay 14 genes que no se pueden detectar con esta herramienta, pero es algo asumible. Aparece una pestaña de Gene Ontology que especifica los distintos niveles de las tres partes (BP, MF y CC). Por ejemplo, para BP, hay 5 niveles, cada uno a un mayor nivel. En direct, se mezclan los distintos niveles para obtener una clasificación que sea detallada, pero sin ser exhaustiva. Como se están testando muchos genes, hay que volver a hacer el ajuste del p-valor, que en este caso aparece en una columna con la corrección de Benjamini. Esto sería el equivalente al ORA.
%Importante describir bien esta parte.

%14/02 - Carlos Torroja
\chapter{ChIP-Seq}
\section{Procedimiento experimental}
ChIP-Seq (Chromatin Immunoprecipitation Sequencing) es una técnica diseñada para identificar las regiones del ADN donde se unen los factores de transcripción. Utiliza secuenciación de nueva generación (NGS) de lecturas cortas (short reads) y se basa en la inmunoprecipitación de factores de transcripción. Si no se dispone de un anticuerpo específico para el factor de transcripción de interés, la técnica no puede llevarse a cabo.

El procedimiento experimental es el siguiente:
\begin{enumerate}
\item \textbf{Fijación y fragmentación del ADN:} Se fijan las células de un tejido con un agente químico para mantener el factor de transcripción unido al ADN. Se extrae el ADN junto con las proteínas unidas y se fragmenta mediante sonicación, obteniendo fragmentos de aproximadamente 200 pares de bases, ideal para la secuenciación con tecnología Illumina. Este tamaño permite además acotar la región donde buscar posteriormente los enriquecimientos.
\item \textbf{Inmunoprecipitación:} Se introduce un anticuerpo específico para el factor de transcripción en la solución de fragmentos de ADN.
Como control, se puede preparar una muestra sin anticuerpo para evaluar la distribución del coverage del genoma.
Los fragmentos de ADN unidos al factor de transcripción se capturan utilizando una columna con bolitas que se unen a la región constante del anticuerpo.
\item \textbf{Preparación de la librería y secuenciación:} Se deshace la fijación para eliminar las proteínas y se prepara la librería de ADN para secuenciar.
Se realiza la secuenciación, idealmente obteniendo entre 20 y 40 millones de lecturas (reads).
\end{enumerate}

\section{Análisis de Datos}
\paragraph{Alineamiento de lecturas}
Las lecturas se alinean al genoma de referencia. En general, se utiliza secuenciación single-end, donde las lecturas no están pareadas.
Se observa un patrón donde las lecturas se alinean en la región 5' en un sentido y en la región 3' en sentido inverso, lo que indica la señal de enriquecimiento.

\paragraph{Identificación de regiones enriquecidas}
Se utilizan herramientas como MACS (Model-based Analysis of ChIP-Seq) para identificar regiones enriquecidas.
MACS modela una distribución nula, fragmenta el genoma en bins y calcula el número de lecturas en cada bin.
La herramienta desplaza y junta los picos de las distribuciones (una por cada sentido) para aumentar la señal y evaluar las regiones enriquecidas en comparación con el control.

\paragraph{Análisis de motivos de unión}
Se identifican las secuencias de ADN donde se une el factor de transcripción y los genes asociados.
Se utilizan herramientas para buscar motivos enriquecidos de 10-15 nucleótidos mediante el algoritmo de expectation-maximization.
Esto permite estimar la secuencia consenso de unión y predecir otros sitios potenciales en el genoma mediante el logo generado.

\section{Aplicación práctica: Pipeline de análisis con Galaxy}
\subsection{Obtención de datos}
Vamos a construir una pipeline para analizar datos de ChIP-Seq del artículo \href{https://www.sciencedirect.com/science/article/pii/S2211124713001368?via\%3Dihub}{"Analysis of the DNA-binding profile and function of tale homeoproteins reveals their specialization and specific interactions with hox genes/proteins"}. Este estudio evalúa los sitios de unión de tres factores de transcripción: Prep1, Meis1 y Pbx1, relacionados con los genes Hox del desarrollo embrionario.

En este estudio se realizaron tres ChIP-Seqs con tres anticuerpos, uno para cada factor. Meis1 salió muy ruidoso, Prep1 tenía unos picos muy marcados, y Pbx1 mostró algo intermedio. Los picos de Meis tenían dos distribuciones, y donde estaba Prep, Meis también tenía un pico marcado. 

Para esta pipeline utilizamos el \href{https://usegalaxy.org/}{servidor americano de Galaxy}. Ahí podemos buscar en historias públicas la historia "Pbx1 ChIPSeq Raw Data" del usuario "cartof", que es Carlos Torroja (el profesor). Los datos del artículo están disponibles en GEO, y podemos acceder desde la sección de "Additional Information" de PubMed. Desde ahí podemos navegar a SRA Run Selector, donde aparecen todas las muestras del artículo. Hay dos tipos de assays, RNA-Seq y ChIP-Seq, por lo que seleccionamos solo aquellas pertenecientes a ChIP-Seq. Hay un botón de "Computing Galaxy" que importa los datos directamente al servidor americano; para utilizar el servidor europeo, habría que descargar las tablas de metadatos e importarlas manualmente como se hizo en la pipeline de RNA-Seq.

\subsection{Preprocesamiento de datos}
Los datos ya están subidos a Galaxy, pero hay que preprocesar el contenido. Las anotaciones de los campos son libres, no hay estándar, por lo que la pipeline se debe adaptar a los datos concretos. 
En este caso, solo nos queremos quedar con tres columnas: la columna 1 run con los identificadores SRR, la columna 17 con el nombre de la librería y la columna 32 con el nombre del anticuerpo utilizado. Para esto, utilizamos la herramienta \texttt{cut columns}. Tras ver el resultado, vemos que algunas celdas contienen "signos prohibidos" en bioinformática, como espacios o paréntesis. Por ello, el siguiente paso es utilizar \texttt{column regex find and replace}. Esto se debe ejecutar dos veces:
\begin{enumerate}
\item \textbf{Columna 2}: debemos realizar dos comprobaciones. Primero, se sustituye .*WT\symbol{92}s([\symbol{92}w\symbol{92}s]+) por \symbol{92}1. El segundo check debe buscar \symbol{92}s y reemplazar por \_.
\item \textbf{Columna 3}: buscamos [\symbol{92}/\symbol{92}s].+ y lo queremos reemplazar por nada, por lo que se deja el campo en blanco.
\end{enumerate}

\subsection{Descarga y extracción de lecturas}
Hasta ahora, hemos realizado los pasos anteriores directamente en la historia. Los siguientes pasos los vamos a crear como parte de un flujo de trabajo, creando un nuevo workflow desde la pestaña con ese mismo nombre. Los flujos de trabajo se construyen con casillas que representan distintas entradas o herramientas, pudiendo ordenar todo.

Primero incluimos una casilla de "input database" que llamamos SRA Table con descripción "Sample table with SRR column and sample name column" y formato tabular. A continuación añadimos la herramienta "cut" con nombre "Extract SRR Column", desactivando los parámetros de la herramienta. De esta forma, a la hora de lanzar el flujo de trabajo, se pide al usuario la introducción de las columnas que se desean mantener. La alternativa sería, en lugar de desactivar el parámetro, generar una entrada que se utilice en la herramienta. Esto se consigue pulsando el icono de las dos flechas, permitiendo así enlazar una casilla de "input" con la herramienta.

La primera fila del fichero contiene la cabecera de las columnas, y no queremos que forme parte del análisis bioinformático. Para ello, incluimos una casilla con la herramienta "select last lines from a database (tail)". Entre los parámetros, debemos especificar "keep everything from this line on" y "2", de manera que siempre se vaya a eliminar la cabecera.

El flujo de trabajo contiene por ahora solo pasos intermedios. Para evitar su aparición en la historia cuando se lance el flujo, generando ruido visual, se pueden ocultar pulsando sobre el símbolo del ojo. Ahora ya podemos incluir la herramienta "Faster Download and Extract Reads in FastQ format". Por defecto, no permite introducirle una entrada, primero hay que seleccionar la opción de "list of SRA accession, one per line". La salida de esta herramienta se divide en cuatro. Para una secuenciación single-end, debemos trabajar con la salida "output\_collection", mientras que para una secuenciación pair-end, "list\_paired".

\begin{figure}[h]
\centering
\includegraphics[width = 0.6\textwidth]{figs/chipseq-workflow-start.png}
\end{figure}

\subsection{Análisis posterior}
Los siguientes pasos del análisis serían realizar un control de calidad con FastQC y utilizar una herramienta de trimmeado (como Trim-Galore) para eliminar los adaptadores de la secuencia, aumentando así la capacidad de alinear. A continuación se podría pasar al alineado para saber la parte del genoma de donde provienen las lecturas mediante herramientas como BWA. En el FastQC se podría ver el tamaño de las regiones, que en este caso son de unos 35 nucleótidos, por lo que se puede utilizar la herramienta BWA estándar al no necesitar incluir gaps u otros eventos complejos más propios de secuencias más largas.

El resultado es un fichero BAM que se utiliza a continuación para realizar un control de calidad con la herramienta \texttt{plotFingerprint}. Del genoma se obtienen fragmentos pequeños que se utilizan para contar las lecturas que caen en cada fragmento. Esto se calcula para todas las muestras, y posteriormente se genera un gráfico que muestre la información: fragmentos por counts y un gráfico cumulativo.

En un análisis de ChIP-Seq, no hay un genoma de referencia, por lo que es necesario incluir un paso de detección de los picos de lectura. Esta detección se realiza en base a un patrón o en base al enriquecimiento. Esto se realiza con la herramienta MACS, la cual se encarga de la detección, del conteo y del análisis estadístico. Se identifican las regiones enriquecidas a través de la distribución de las lecturas. Se deben encontrar dos distribuciones, una para cada sentido de lectura. MACS puede detectar una distribución y emplearla para buscar una distribución opuesta en el rango de la sonicación (es decir, unos 200 pares de bases que se especifican como parámetro). A continuación junta ambas distribuciones en un pico central para aumentar la detección. 
%Para definir un tránscrito, debe estar enriquecido y cumplir con un patrón. En el caso de los picos, la detección es simplemente "enrichment-based". El transcriptoma debe ser compatible con las características que queremos detectar (miRNA, mRNA, sitio de unión a factor de transcripción). 
El siguiente paso es la búsqueda en distintos tamaños de fragmento, quedándose con el que mejor se adapte y simulando la distribución nula con los datos. Además, se pueden modificar el tamaño efectivo del genoma de referencia e incluir en las salidas adicionales la de "peak summits". 

El resultado es un fichero bed que se debe ordenar según el score. Este fichero se emplea para la búsqueda de los motivos de unión del factor de transcripción. En el fichero bed se incluyen las posiciones del nucleótido donde empieza y donde termina, por lo que debemos aumentar el número en 50 nucleótidos en ambas direcciones (restar 50 a la posición de inicio y sumar 50 a la de fin). De la colección de regiones, se pueden filtrar las 500 entradas más altas y utilizar la herramienta "Extract Genomic DNA" para obtener la secuencia de esas posiciones. Con esto se puede utilizar la herramienta "MEME", obteniendo así los logos. Pulsando la flecha en Submit/Download, podemos buscar ese logo en la base de datos para encontrar otras secuencias iguales recogidas en ella.

%09/04 - Carlos Torroja
\section{Single Cell RNA-Seq}
Hasta ahora, para explorar el estado fisiológico de las células era mirar el perfil transcripcional mediante homogeneizados de tejidos. Los cambios se pueden notar, pero no se puede determinar exactamente qué célula ha cambiado. No se pueden asociar cambios transcripcionales a funciones específicas. En experimentos de célula única, sí se puede determinar la abundancia de los tipos celulares, su expresión y cambios. 

\subsection{Consideraciones de diseño experimental}
Single cell apareció en 2009, pero no fue hasta 2014 cuando empezó a aparecer la tecnología que permitió que fuera asequible y generalizable. Cuando antiguamente se debían separar las células de forma manual, en 2015 se utilizaban nanoburbujas, picowells y finalmente barcoding \textit{in situ}. Esta última es una combinación entre la multiplexación y splitseq, poniendo las células en pocillos donde se permeabilizan permitiendo que entren ciertos adaptadores para empezar a marcar el transcriptoma. Con tres ciclos de separación, es difícil que las células se mantengan en el mismo pool durante los tres ciclos, pudiendo así diferenciarlas. 

Un experimento de single cell se utiliza para identificar poblaciones celulares en un modelo experimental, aumentar el conocimiento de poblaciones celulares ya conocidas o poder comparar poblaciones en distintas condiciones o desarrollos. 

Cuando se prepara un experimento de Single Cell, se deben tener en cuenta muchos aspectos. Primero se debe elegir el tejido del cual obtener la información. La sangre es muy fácil en términos de aislamiento de células, pero otros tejidos pueden tener distintas estrategias para disgregar las células. El diseño es importante para reducir coste, pero con una gran obtención de información útil y precisa. Para ello se requiere de una pregunta clara de lo que se quiere conseguir y conocer la tecnología para reducir errores y sesgos. 

Los principios de experimentación son replicación, randomización y blocking. Cada parte está encaminada a eliminar sesgos y capturar la variabilidad biológica medida. Entre los errores puede haber errores aleatorios que no se pueden controlar y errores sistemáticos que sí son controlables a través del blocking. A la hora de randomización, hay que tener en cuenta los distintos niveles de sampleado. En Single Cell, existe el sampleado de sujeto y un sampleo de células. Dentro de la célula se hace otro sampleo a nivel molecular al no capturar todo el ARN. Tras capturar el ARN, se secuencian fragmentos, por lo que hay otro tipo de sampleado más.
En algunos casos, el sampleo puede sufrir de sesgos biológicos y en otros técnicos, siendo importante realizar réplicas de ambos tipos. 

Ejemplo: realizamos 48 medidas y debemos medir la expresión de un gen. Hay que ver cómo diseñar el experimento: número de ratones, número de células y número de mediciones por célula. En Single Cell esto es importante porque hay experimentalistas que toman una medida por célula por tener muchas células, pero ahí no se evalúa la variabilidad biológica a nivel de individuo. En  \href{https://www.nature.com/articles/nmeth.3091}{el siguiente paper} se midieron la expresión de un gen con distintas réplicas. Teniendo en cuenta las réplicas animales, celulares y medidas, la varianza total depende de la varianza de estas tres réplicas. Los autores simularon con las distribuciones el experimento en distintas condiciones. En un primer caso, se coge un animal y 48 células, dos animales y 24 células, 4 animales y 12 células, etc. Para cada situación, los autores sacan la variabilidad que les da. Con un solo ratón, no hay variabilidad a nivel de ratón. Conforme se toman más ratones, la variabilidad aumenta hasta llegar a la variabilidad total real estimada antes. Cuando se mide la variabilidad de la media de expresión, al principio las estimaciones son pésimas, y conforme se añaden réplicas biológicas, la variabilidad disminuye. Cuando se realizan 3 mediciones por célula en lugar de 1, la estimación de la media es mayor que en el caso anterior. Es mejor hacer réplicas biológicas en medida de lo posible y no tener en cuenta la variabilidad técnica, ya que está implícita también en la biológica. 
A la hora de replicar, se deben tener al menos tres réplicas biológicas en las tecnologías ómicas, ya que es cuando el estadístico t disminuye considerablemente. 

Otra pregunta clave es la cantidad a secuenciar. Hay una serie de simulaciones que miden la cantidad de genes diferencialmente expresados obtenidos con la profundidad de las lecturas. Hay veces en las que aumentar la profundidad no afecta a los genes diferencialmente expresados detectados, y es mejor aumentar las réplicas para poder detectar más genes incluso con una profundidad un poco menor. 

En general, no se deben hacer pooles, eso se hace en secuenciación bulk. Si se hacen, se debe comprobar que no estén desbalanceados. Además, deben ser todos del mismo sexo, o al menos estar dos y dos, ya que puede ser importante a la hora de calcular los diferenciales. 

¿Cuántas células se deben secuenciar? Las menos posibles (cada célula cuesta un dinero) con una información la mayor posible. En algunos casos, se separa un tipo celular de interés para poder extraer información más específica (por ejemplo, coger una muestra de corazón y enriquecer los macrófagos para secuenciar solo ese tipo de células). El programa SCOPIT puede ayudar a determinar este número. 
Con un presupuesto fijo, lo óptimo es secuenciar el máximo número de células con una lectura por célula por gen. 

En conclusión, hay que pensar sobre el diseño experimental antes de empezar a desarrollar el experimento. Hay que intentar tener réplicas, mejor si son biológicas (no deberíamos perder el tiempo con las réplicas técnicas). Se debe reducir el pool lo máximo posible y estimar el número de céluas base mínimo con el conocimiento previo.

\subsection{Librerías}
Del transcriptoma, con otras librerías se puede obtener también otra información valiosa: modificaciones de histonas, accesibilidad de la cromatina, secuencia del genoma, metilación del ADN, posición espacial, etc.

Cuando se trabaja con un tejido sólido, se trata con una pronasa (mezcla de tripsina y colagenasa), parar la reacción, lavar y triturar la muestra. La sangre no tiene un proceso tan largo, ya que se pueden digerir los eritrocitos y ya. 

Hay dos formas de preparar la librería. Hay un \textbf{método 3’ (scRNA-Seq 3’ end)} en el que se usan beads (perlas) recubiertas con oligos. Cada oligo en la bead tiene una cola poliT para unirse a la cola poliA del mRNA, un código de barra celular (cell barcode) que identifica de qué célula proviene ese mRNA y un UMI (Unique Molecular Identifier), que es una etiqueta única para cada transcripto individual, usada para contar copias de manera precisa (evitando duplicados por PCR). El oligo en la bead actúa como primer. Se une directamente al mRNA por complementariedad (poliT - poliA). Luego, con transcriptasa reversa, se genera cDNA (ADN complementario) a partir del mRNA.
En cuanto al \textbf{método 5’ (scRNA-Seq 5’ end),} la bead no tiene una cola poliT. El primer inicial (que se une al mRNA) es libre en solución y tiene una cola poliA artificial. Lo que tiene la bead es un TSO (Template Switch Oligo). El primer suelto se une a una región interna del mRNA o al cap del 5’. La transcriptasa reversa copia el mRNA, y cuando llega al final del ARN, hace lo que se llama un template switch: “salta” y comienza a copiar también una secuencia del TSO que está en la bead. Este TSO contiene: un código de barra celular, un UMI y secuencias necesarias para amplificación posterior.

Se pueden usar anticuerpos añadidos a la secuenciación para marcar las células. Los anticuerpos tienen un PCR Handle, un barcode específico del anticuerpo y una secuencia flanqueante. Así, se puede medir la cantidad de proteína en una célula. Cuantos más antígenos de superficie haya, más anticuerpos se unen y más veces se detecta ese barcode. 

ATAC-Seq permite detectar las regiones del genoma activas transcripcionalmente. Utiliza una transposasa que corta el genoma a la vez que le mete secuencias (adaptadores). 

DOGMA-Seq combina la captura del ADN, ARN y proteínas, es decir, ATAC, anticuerpos. 

\subsection{Spatial Single Cell Transcriptomics}
Recientemente se busca evaluar transcripcionalmente las células en su situación en el tejido. Hay muchas técnicas, pero se dividen en dos: basadas en imagen o basadas en secuenciación.

Las basadas en imagen tienen una resolución subcelular. Mediante una batería de oligos se marcan los transcritos. Los oligos tienen unas secuencias específicas pegadas construidas de forma que, en varios ciclos de secuenciación, con estas secuencias se vea la combinatoria. La sonda se une a la secuencia y por fluorescencia se ven los tránscritos a los que se ha unido. Esto se repite con otras sondas. Por ello, en la generación de oligos, se tiene en cuenta la combinación de las secuencias de hibridación a la sonda. Se captura in situ cuántas moléculas son de un gen específico. 

La detección basada en secuenciación se basa en un slide donde se pegan las secuencias a los microarrays. En cada punto se conoce el barcode que hay, teniendo así un marcador de la posición. Dependiendo de la tecnología, la resolución puede ser de varias células o subcelular. El tejido se pone encima del porta, se permeabiliza y el ARN se pega en el spot. 

Aquellos métodos basados en secuenciación pueden capturar un mayor número de genes. La transcriptómica espacial single-cell permite estudiar la comunicación intercelular en función de los genes expresados y su localización. Las interacciones se pueden buscar en base a su posición, por lo que el nivel de información es mayor y más preciso. 

%23/04 - Carlos Torroja
Cada librería requiere de una longitud diferente.

\subsection{Procesamiento y análisis de datos}
\subsubsection{Alineamiento y cuantificación}
R2 se anota con el barcode de la célula y UMI de R1, y desde entonces se utiliza solo R2. Se mapea al genoma con STAR. Para los siguientes pasos de recuento de UMI sólo se tienen en cuenta las alineaciones compatibles con transcripciones asociadas a un único gen. En la cuantificación de UMIs, se tiene una combinación entre barcode, UMI y gen. Las combinaciones idénticas se colapsan a una copia al tratarse de duplicados de PCR, y se computa así los UMI por gen por célula.

Un UMI está asociado a una molécula de mRNA. Se puede observar el mismo UMI unido a distintos framentos del mismo mRNA. Estos fragmentos distintos se deben a la fragmentación enzimática en la amplificación de PCR. Todos los UMIs idénticos se han originado de la misma molécula de mRNA individual.

\subsubsection{Detección de células}
Las beads están vacías en su mayoría. Para no provocar que dos células juntas estén anotadas, pudiendo mezclarse los tránscritos, se espacian las células con beads. El bead captura el RNA que se amplifica posteriormente. Se debe discernir cuando la burbuja está vacía y cuando no. Una vez con las células cuantificadas, se evalúa la cantidad de UMIs detectados en cada una. Esto se hace en dos pasos. Primero se coloca un punto de corte en el que los recuentos por celda descienden drásticamente. En las células de menor rendimiento se elabora un modelo del fondo basado en los propios datos y las células se llaman cuando difieren sustancialmente de ese modelo. 

Los datos de single-cell son muy escasos, y la correlación entre réplicas es muy pobre, al contrario que en secuenciación bulk. 

\subsection{Práctica Comprehensive integration of single-cell data}
Se utilizarán los datos de \href{https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE128639}{GSE128639 en GEO}. Se analizaron muestras de sangre que se incubaron con ADT (anticuerpos). De ahí se tomaron alícuotas y se incubaron con tags de anticuerpos (HTO). Se realizaron pools de las alícuotas, se capturaron las células, se extrajo el RNA y se generó el cDNA. Con eso se prepararon 3 librerías: expresión génica, captura de anticuerpo (ADT) y los tags HTO. Esto se secuencia y se procesa para cuantificar.

Recibimos un fastq de cada una de las tres librerías. Con la herramienta cellranger se realiza la cuantificación. También se podría ver la cromatina abierta con otros protocolos. 

\begin{lstlisting}[language=bash]
cellranger count --id=GSE128639_10XGenomics \
	--libraries=GSE128639_10XGenomics_Sample_fastqs.csv \
	--transcriptome=refdata-gex-GRCh38-2020-A \
	--feature-ref=GSE128639_10XGenomics_HTORefTable_feature_ref.csv \
	--chemistry=threeprime --localcores=40 --localmem=96 --disable-ui --nosecondary --no-bam
\end{lstlisting}

La salida es una serie de tablas, generando así la sparse matrix. En la matrix market, cada posición está definida por tres números (coordenada x, y y el valor; posición, célula y contaje). Esto se combina con los barcodes y las características para que la sparse matrix contenga el ID del gen y su contaje. 

El resultado se cellranger se puede ver en un HTML (carpeta prácticas > SingleCellRNA > GSE12839 \_10XGenomics > outs > web\_summary.html). El resto de la práctica se realiza en R (carpeta prácticas > SingleCellRNA > script.Rmd).

%25/04 - Carlos Torroja
\subsection{Filtrado}
A pesar de que CellRanger funcione bien para dar células reales, hay veces en las que el experimento no ha salido del todo bien y pueden quedarse todavía células no reales residuales en los datos, por lo que se deben filtrar. 

Se filtran células con pocos counts. Dependiendo del experimento y de las células, se establecen los umbrales, pero se suele hacer un poco "a ojo". El filtrado se ha ejecutado en la carpeta de prácticas.

Tras el filtrado se realiza el demultiplexado. Lo más habitual es hacer la misma normalización que con RNA-Seq con TPMs. No obstante, hay también otros métodos. Por ejemplo, se puede normalizar por pooles. Aleatoriamente, se coge un determinado número de células y se suman sus counts. Así, cada célula está representada en distintos pooles con distintas células, generando un perfil. Estos pooles se normalizan con un pool total hecho con todas las células. Para cada célula quedan distintos pooles, que se representan en sistemas de ecuaciones que se pueden resolver obteniendo los factores de normalización. Esto se parece mucho a la división por counts totales, pero está algo más protegido a la cantidad de genes en cada célula. Esta normalización es así más robusta. 

Realizaremos esto en código, siguiendo con el script. 

%Identificación de tipos celulares
%Comunicación intercelular
%Análisis de trayectoria o pseudotiempo, interactoma, reguloma



